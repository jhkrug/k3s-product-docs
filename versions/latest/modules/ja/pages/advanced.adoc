= 高度なオプション / 設定
:revdate: 2025-06-23
:page-revdate: {revdate}

このセクションには、K3sを実行および管理するさまざまな方法や、K3sの使用に向けてホストOSを準備するために必要な手順についての高度な情報が含まれています。

== 証明書管理

=== 証明書認証局 (CA) 証明書

K3sは、最初のサーバーノードの起動時に自己署名の証明書認証局 (CA) 証明書を生成します。これらのCA証明書は10年間有効であり、自動的に更新されません。

カスタムCA証明書の使用や自己署名CA証明書の更新についての情報は、xref:cli/certificate.adoc#_certificate_authority_ca_certificates[`k3s certificate rotate-ca` コマンドのドキュメント]を参照してください。

=== クライアントおよびサーバー証明書

K3sのクライアントおよびサーバー証明書は、発行日から365日間有効です。期限が切れているか、期限が切れるまで90日以内の証明書は、K3sが起動するたびに自動的に更新されます。

クライアントおよびサーバー証明書を手動で回転させる方法については、xref:cli/certificate.adoc#_client_and_server_certificates[`k3s certificate rotate` コマンドのドキュメント]を参照してください。

== トークン管理

デフォルトでは、K3sはサーバーとエージェントの両方に対して単一の静的トークンを使用します。With care, this token can be rotated once the cluster has been created.
It is also possible to enable a second static token that can only be used to join agents, or to create temporary `kubeadm` style join tokens that expire automatically.
詳細については、xref:cli/token.adoc#_k3s_token_1[`k3s token` コマンドのドキュメント]を参照してください。

== HTTPプロキシの設定

K3sをHTTPプロキシを介してのみ外部接続が可能な環境で実行している場合、K3sのsystemdサービスでプロキシ設定を構成できます。これらのプロキシ設定はK3sで使用され、埋め込まれたcontainerdおよびkubeletに渡されます。

[NOTE]
====
Proxy configuration and other environment variables from the host are NOT passed into Pods.
====

K3sのインストールスクリプトは、現在のシェルに存在する場合、`HTTP_PROXY`、`HTTPS_PROXY`、`NO_PROXY`、および `CONTAINERD_HTTP_PROXY`、`CONTAINERD_HTTPS_PROXY`、`CONTAINERD_NO_PROXY` 変数を自動的に取得し、通常は次の環境ファイルに書き込みます：

* `/etc/systemd/system/k3s.service.env`
* `/etc/systemd/system/k3s-agent.service.env`

もちろん、これらのファイルを編集してプロキシを設定することもできます。

K3sはクラスター内部のPodおよびサービスIP範囲とクラスターDNSドメインを `NO_PROXY` エントリのリストに自動的に追加します。Kubernetesノード自体が使用するIPアドレス範囲（つまり、ノードのパブリックおよびプライベートIP）が `NO_PROXY` リストに含まれているか、ノードがプロキシを介して到達可能であることを確認する必要があります。

----
HTTP_PROXY=http://your-proxy.example.com:8888
HTTPS_PROXY=http://your-proxy.example.com:8888
NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
----

containerdのプロキシ設定をK3sおよびKubeletに影響を与えずに構成したい場合は、変数に `CONTAINERD_` をプレフィックスとして付けることができます：

----
CONTAINERD_HTTP_PROXY=http://your-proxy.example.com:8888
CONTAINERD_HTTPS_PROXY=http://your-proxy.example.com:8888
CONTAINERD_NO_PROXY=127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
----

== Dockerをコンテナランタイムとして使用する

K3sには業界標準のコンテナランタイムであるlink:https://containerd.io/[containerd]が含まれており、デフォルトで使用されます。
Kubernetes 1.24以降、Kubeletにはdockershimが含まれておらず、Kubeletがdockerdと通信するためのコンポーネントがありません。
K3s 1.24以降にはlink:https://github.com/Mirantis/cri-dockerd[cri-dockerd]が含まれており、Dockerコンテナランタイムを引き続き使用しながら、以前のリリースからシームレスにアップグレードできます。

Dockerをcontainerdの代わりに使用するには：

. K3sノードにDockerをインストールします。Rancherのlink:https://github.com/rancher/install-docker[Dockerインストールスクリプト]の1つを使用してDockerをインストールできます：
+
[,bash]
----
curl https://releases.rancher.com/install-docker/20.10.sh | sh
----

. `--docker` オプションを使用してK3sをインストールします：
+
[,bash]
----
curl -sfL https://get.k3s.io | sh -s - --docker
----

. クラスターが利用可能であることを確認します：
+
[,bash]
----
$ sudo k3s kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY   STATUS      RESTARTS   AGE
kube-system   local-path-provisioner-6d59f47c7-lncxn   1/1     Running     0          51s
kube-system   metrics-server-7566d596c8-9tnck          1/1     Running     0          51s
kube-system   helm-install-traefik-mbkn9               0/1     Completed   1          51s
kube-system   coredns-8655855d6-rtbnb                  1/1     Running     0          51s
kube-system   svclb-traefik-jbmvl                      2/2     Running     0          43s
kube-system   traefik-758cd5fc85-2wz97                 1/1     Running     0          43s
----

. Dockerコンテナが実行中であることを確認します：
+
[,bash]
----
$ sudo docker ps
CONTAINER ID        IMAGE                     COMMAND                  CREATED              STATUS              PORTS               NAMES
3e4d34729602        897ce3c5fc8f              "entry"                  About a minute ago   Up About a minute                       k8s_lb-port-443_svclb-traefik-jbmvl_kube-system_d46f10c6-073f-4c7e-8d7a-8e7ac18f9cb0_0
bffdc9d7a65f        rancher/klipper-lb        "entry"                  About a minute ago   Up About a minute                       k8s_lb-port-80_svclb-traefik-jbmvl_kube-system_d46f10c6-073f-4c7e-8d7a-8e7ac18f9cb0_0
436b85c5e38d        rancher/library-traefik   "/traefik --configfi…"   About a minute ago   Up About a minute                       k8s_traefik_traefik-758cd5fc85-2wz97_kube-system_07abe831-ffd6-4206-bfa1-7c9ca4fb39e7_0
de8fded06188        rancher/pause:3.1         "/pause"                 About a minute ago   Up About a minute                       k8s_POD_svclb-traefik-jbmvl_kube-system_d46f10c6-073f-4c7e-8d7a-8e7ac18f9cb0_0
7c6a30aeeb2f        rancher/pause:3.1         "/pause"                 About a minute ago   Up About a minute                       k8s_POD_traefik-758cd5fc85-2wz97_kube-system_07abe831-ffd6-4206-bfa1-7c9ca4fb39e7_0
ae6c58cab4a7        9d12f9848b99              "local-path-provisio…"   About a minute ago   Up About a minute                       k8s_local-path-provisioner_local-path-provisioner-6d59f47c7-lncxn_kube-system_2dbd22bf-6ad9-4bea-a73d-620c90a6c1c1_0
be1450e1a11e        9dd718864ce6              "/metrics-server"        About a minute ago   Up About a minute                       k8s_metrics-server_metrics-server-7566d596c8-9tnck_kube-system_031e74b5-e9ef-47ef-a88d-fbf3f726cbc6_0
4454d14e4d3f        c4d3d16fe508              "/coredns -conf /etc…"   About a minute ago   Up About a minute                       k8s_coredns_coredns-8655855d6-rtbnb_kube-system_d05725df-4fb1-410a-8e82-2b1c8278a6a1_0
c3675b87f96c        rancher/pause:3.1         "/pause"                 About a minute ago   Up About a minute                       k8s_POD_coredns-8655855d6-rtbnb_kube-system_d05725df-4fb1-410a-8e82-2b1c8278a6a1_0
4b1fddbe6ca6        rancher/pause:3.1         "/pause"                 About a minute ago   Up About a minute                       k8s_POD_local-path-provisioner-6d59f47c7-lncxn_kube-system_2dbd22bf-6ad9-4bea-a73d-620c90a6c1c1_0
64d3517d4a95        rancher/pause:3.1         "/pause"
----

== etcdctlの使用

etcdctlは、etcdサーバーと対話するためのCLIを提供します。K3sにはetcdctlはバンドルされていません。

K3sの埋め込みetcdと対話するためにetcdctlを使用したい場合は、https://etcd.io/docs/latest/install/[公式ドキュメント]を使用してetcdctlをインストールしてください。

[,bash]
----
ETCD_VERSION="v3.5.5"
ETCD_URL="https://github.com/etcd-io/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz"
curl -sL ${ETCD_URL} | sudo tar -zxv --strip-components=1 -C /usr/local/bin
----

次に、K3s管理の証明書とキーを認証に使用するようにetcdctlを構成して使用できます：

[,bash]
----
sudo etcdctl version \
  --cacert=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt \
  --cert=/var/lib/rancher/k3s/server/tls/etcd/client.crt \
  --key=/var/lib/rancher/k3s/server/tls/etcd/client.key
----

== containerdの設定

[IMPORTANT]
.Version Gate
====
K3s includes containerd 2.0 as of the February 2025 releases: v1.31.6+k3s1 and v1.32.2+k3s1.  
Be aware that containerd 2.0 prefers config version 3, while containerd 1.7 prefers config version 2.
====

K3s will generate a configuration file for containerd at `/var/lib/rancher/k3s/agent/etc/containerd/config.toml`, using values specific to the current cluster and node configuration.

For advanced customization, you can create a containerd config template in the same directory:

* For containerd 2.0, place a version 3 configuration template in `config-v3.toml.tmpl`.
+
See the https://github.com/containerd/containerd/blob/release/2.0/docs/cri/config.md[containerd 2.0 documentation] for more information.
* For containerd 1.7 and earlier, place a version 2 configuration template in `config.toml.tmpl`.
+
See the https://github.com/containerd/containerd/blob/release/1.7/docs/cri/config.md[containerd 1.7 documentation] for more information.

Containerd 2.0 is backwards compatible with prior config versions, and k3s will continue to render legacy version 2 configuration from `config.toml.tmpl` if `config-v3.toml.tmpl` is not found.

The template file is rendered into the containerd config using the https://pkg.go.dev/text/template[`text/template`] library.
See `ContainerdConfigTemplateV3` and `ContainerdConfigTemplate` in https://github.com/k3s-io/k3s/blob/master/pkg/agent/templates/templates.go[`templates.go`] for the default template content.
The template is executed with a https://github.com/k3s-io/k3s/blob/master/pkg/agent/templates/templates.go#L22-L33[`ContainerdConfig`] struct as its dot value (data argument).

=== ベーステンプレート

You can extend the K3s base template instead of copy-pasting the complete stock template out of the K3s source code. This is useful if you only need to build on the existing configuration by adding a few extra lines before or after the defaults.

[,toml]
----
#/var/lib/rancher/k3s/agent/etc/containerd/config-v3.toml.tmpl

{{ template "base" . }}

[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.'custom']
  runtime_type = "io.containerd.runc.v2"
[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.'custom'.options]
  BinaryName = "/usr/bin/custom-container-runtime"
  SystemdCgroup = true
----

[WARNING]
====
For best results, do NOT simply copy a prerendered `config.toml` into the template and make your desired changes. Use the base template, or provide a full template based on the k3s defaults linked above.
====

== Alternativeコンテナランタイムのサポート

K3s will automatically detect alternative container runtimes if they are present when K3s starts. Supported container runtimes are:

----
crun, lunatic, nvidia, nvidia-cdi, nvidia-experimental, slight, spin, wasmedge, wasmer, wasmtime, wws
----

NVIDIA GPUs require installation of the NVIDIA Container Runtime in order to schedule and run accelerated workloads in Pods. To use NVIDIA GPUs with K3s, perform the following steps:

. ノードにnvidia-containerパッケージリポジトリをインストールします。手順は以下を参照してください：
 https://nvidia.github.io/libnvidia-container/
. nvidiaコンテナランタイムパッケージをインストールします。例えば：
`apt install -y nvidia-container-runtime cuda-drivers-fabricmanager-515 nvidia-headless-515-server`
. xref:installation/installation.adoc[Install K3s], or restart it if already installed.
. Confirm that the nvidia container runtime has been found by k3s: 
`grep nvidia /var/lib/rancher/k3s/agent/etc/containerd/config.toml`

K3s includes Kubernetes RuntimeClass definitions for all supported alternative runtimes. You can select one of these to replace `runc` as the default runtime on a node by setting the `--default-runtime` value via the k3s CLI or config file.

If you have not changed the default runtime on your GPU nodes, you must explicitly request the NVIDIA runtime by setting `runtimeClassName: nvidia` in the Pod spec:

[,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
----

NVIDIA Container Runtime は https://github.com/NVIDIA/k8s-device-plugin/[NVIDIA Device Plugin] と頻繁に使用され、上記のように pod スペックに `runtimeClassName: nvidia` を含めるように変更されることが多いことに注意してください。

[#_running_agentless_servers_experimental]
== エージェントレスサーバーの実行 (実験的機能)

____
*警告:* この機能は実験的です。
____

`--disable-agent` フラグを使用して開始すると、サーバーは kubelet、コンテナランタイム、または CNI を実行しません。クラスターに Node リソースを登録せず、`kubectl get nodes` の出力には表示されません。
kubelet をホストしないため、Pod を実行したり、クラスターのノードを列挙するオペレーター（埋め込みの etcd コントローラーやシステムアップグレードコントローラーを含む）によって管理されたりすることはできません。

エージェントレスサーバーを実行することは、エージェントやワークロードからコントロールプレーンノードを発見されないようにする場合に有利ですが、クラスターオペレーターのサポートがないため管理の負担が増加します。

デフォルトでは、エージェントレスサーバーの apiserver はクラスター内で実行されているアドミッションウェブフックや集約 API サービスへの外向き接続を行うことができません。これを解決するには、`--egress-selector-mode` サーバーフラグを `pod` または `cluster` に設定します。既存のクラスターでこのフラグを変更する場合、オプションが有効になるためにはクラスター内のすべてのノードを再起動する必要があります。

== ルートレスサーバーの実行 (実験的機能)

____
*警告:* この機能は実験的です。
____

ルートレスモードでは、K3s サーバーを特権のないユーザーとして実行できるため、ホストの実際の root を潜在的なコンテナブレークアウト攻撃から保護できます。

ルートレス Kubernetes について詳しくは https://rootlesscontaine.rs/ を参照してください。

[#_known_issues_with_rootless_mode]
=== ルートレスモードの既知の問題

* *ポート*
+
ルートレスで実行すると、新しいネットワーク名前空間が作成されます。これは、K3s インスタンスがホストからかなり分離されたネットワークで実行されることを意味します。
ホストから K3s で実行されているサービスにアクセスする唯一の方法は、K3s ネットワーク名前空間へのポートフォワードを設定することです。
ルートレス K3s には、6443 および 1024 未満のサービスポートをホストにオフセット 10000 で自動的にバインドするコントローラーが含まれています。
+
例えば、ポート 80 のサービスはホスト上で 10080 になりますが、8080 はオフセットなしで 8080 になります。現在、自動的にバインドされるのは LoadBalancer サービスのみです。

* *Cgroups*
+
Cgroup v1 およびハイブリッド v1/v2 はサポートされていません。純粋な Cgroup v2 のみがサポートされています。ルートレスで実行中に K3s が cgroups の欠如により起動に失敗する場合、ノードがハイブリッドモードになっており、「欠落している」cgroups が v1 コントローラーにまだバインドされている可能性があります。

* *マルチノード/マルチプロセスクラスター*
+
マルチノードのルートレスクラスターや同じノード上での複数のルートレス k3s プロセスは現在サポートされていません。詳細については https://github.com/k3s-io/k3s/issues/6488#issuecomment-1314998091[#6488] を参照してください。

=== ルートレスサーバーの開始

* cgroup v2 デリゲーションを有効にします。詳細は https://rootlesscontaine.rs/getting-started/common/cgroup2/ を参照してください。
このステップは必須です。適切な cgroups がデリゲートされていないと、ルートレス kubelet は起動に失敗します。
* https://github.com/k3s-io/k3s/blob/master/k3s-rootless.service[`+https://github.com/k3s-io/k3s/blob/<VERSION>/k3s-rootless.service+`] から `k3s-rootless.service` をダウンロードします。
`k3s-rootless.service` と `k3s` のバージョンが同じであることを確認してください。
* `k3s-rootless.service` を `~/.config/systemd/user/k3s-rootless.service` にインストールします。
このファイルをシステム全体のサービス (`+/etc/systemd/...+`) としてインストールすることはサポートされていません。
`k3s` バイナリのパスに応じて、ファイルの `+ExecStart=/usr/local/bin/k3s ...+` 行を変更する必要があるかもしれません。
* `systemctl --user daemon-reload` を実行します。
* `systemctl --user enable --now k3s-rootless` を実行します。
* `KUBECONFIG=~/.kube/k3s.yaml kubectl get pods -A` を実行し、Pod が実行されていることを確認します。

____
*注意:* ターミナルで `k3s server --rootless` を実行しようとしないでください。ターミナルセッションでは cgroup v2 デリゲーションが許可されていません。
どうしてもターミナルで試す必要がある場合は、`systemd-run --user -p Delegate=yes --tty k3s server --rootless` を使用して systemd スコープでラップしてください。
____

=== 高度なルートレス設定

ルートレス K3s は、ホストとユーザーネットワーク名前空間間の通信に https://github.com/rootless-containers/rootlesskit[rootlesskit] と https://github.com/rootless-containers/slirp4netns[slirp4netns] を使用します。
rootlesskit と slirp4netns によって使用される一部の設定は環境変数で設定できます。これらを設定する最良の方法は、k3s-rootless systemd ユニットの `Environment` フィールドに追加することです。

|===
| 変数名 | デフォルト値 | 説明

| `K3S_ROOTLESS_MTU`
| 1500
| slirp4netns 仮想インターフェースの MTU を設定します。

| `K3S_ROOTLESS_CIDR`
| 10.41.0.0/16
| slirp4netns 仮想インターフェースで使用される CIDR を設定します。

| `K3S_ROOTLESS_ENABLE_IPV6`
| 自動検出
| slirp4netns の IPv6 サポートを有効にします。指定されていない場合、K3s がデュアルスタック操作に設定されている場合に自動的に有効になります。

| `K3S_ROOTLESS_PORT_DRIVER`
| builtin
| ルートレスポートドライバーを選択します。`builtin` または `slirp4netns` のいずれかです。builtin は高速ですが、受信パケットの元の送信元アドレスを偽装します。

| `K3S_ROOTLESS_DISABLE_HOST_LOOPBACK`
| true
| ゲートウェイインターフェースを介してホストのループバックアドレスへのアクセスを有効にするかどうかを制御します。セキュリティ上の理由から、これを変更しないことをお勧めします。
|===

=== ルートレスのトラブルシューティング

* `systemctl --user status k3s-rootless` を実行してデーモンのステータスを確認します。
* `journalctl --user -f -u k3s-rootless` を実行してデーモンログを確認します。
* 詳細は https://rootlesscontaine.rs/ を参照してください。

== ノードラベルとテイント

K3s エージェントは、kubelet にラベルとテイントを追加するオプション `--node-label` および `--node-taint` で構成できます。これらのオプションは xref:cli/agent.adoc#_node_labels_and_taints_for_agents[登録時] にのみラベルおよび/またはテイントを追加するため、ノードがクラスターに最初に参加する際にのみ設定できます。

現在のすべての Kubernetes バージョンでは、`kubernetes.io` および `k8s.io` プレフィックスを持つほとんどのラベルでノードの登録が制限されています。特に `kubernetes.io/role` ラベルが含まれます。許可されていないラベルでノードを起動しようとすると、K3s は起動に失敗します。Kubernetes の著者によると:

____
ノードは自分自身の役割ラベルを主張することは許可されていません。ノードの役割は通常、特権またはコントロールプレーンタイプのノードを識別するために使用され、ノードが自分自身をそのプールにラベル付けすることを許可すると、侵害されたノードが高い特権の資格情報にアクセスするワークロード（コントロールプレーンデーモンセットなど）を簡単に引き付けることができます。
____

詳細については https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/279-limit-node-access/README.md#proposal[SIG-Auth KEP 279] を参照してください。

ノードの登録後にノードラベルとテイントを変更したり、予約済みラベルを追加したりする場合は、`kubectl` を使用する必要があります。テイントの追加方法については公式の Kubernetes ドキュメントを参照してください。link:https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/[テイント] および https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node[ノードラベル] の詳細を参照してください。

== インストールスクリプトでサービスを開始する

インストールスクリプトは、OS が systemd または openrc を使用しているかを自動検出し、インストールプロセスの一環としてサービスを有効化および開始します。

* openrc で実行する場合、ログは `/var/log/k3s.log` に作成されます。
* systemd で実行する場合、ログは `/var/log/syslog` に作成され、`journalctl -u k3s`（エージェントの場合は `journalctl -u k3s-agent`）を使用して表示されます。

インストールスクリプトで自動起動およびサービスの有効化を無効にする例:

[,bash]
----
curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_ENABLE=true sh -
----

[#_running_k3s_in_docker]
== Docker で K3s を実行する

Docker で K3s を実行する方法はいくつかあります:

[tabs]
======
K3d::
+
--
https://github.com/k3d-io/k3d[k3d] は、Docker でマルチノード K3s クラスターを簡単に実行するために設計されたユーティリティです。

k3d を使用すると、ローカルでの Kubernetes 開発のために、Docker でシングルノードおよびマルチノードの k3s クラスターを非常に簡単に作成できます。

インストール方法や k3d の使用方法については、https://k3d.io/#installation[インストール] ドキュメントを参照してください。
--

Docker::
+
--
Docker を使用するには、K3s サーバーおよびエージェントを実行するための `rancher/k3s` イメージも利用可能です。
`docker run` コマンドを使用して:

[,bash]
----
sudo docker run \
  --privileged \
  --name k3s-server-1 \
  --hostname k3s-server-1 \
  -p 6443:6443 \
  -d rancher/k3s:v1.24.10-k3s1 \
  server
----

[NOTE]
====
有効な K3s バージョンをタグとして指定する必要があります。`latest` タグは維持されていません。
Docker イメージではタグに `+` 記号を使用できないため、代わりに `-` を使用してください。
====

K3s が起動して実行されると、管理用 kubeconfig を Docker コンテナからコピーして使用できます:

[,bash]
----
sudo docker cp k3s-server-1:/etc/rancher/k3s/k3s.yaml ~/.kube/config
----
--
======

[#_selinux_support]
== SELinux サポート

SELinux がデフォルトで有効になっているシステム（CentOS など）に K3s をインストールする場合、適切な SELinux ポリシーがインストールされていることを確認する必要があります。

[tabs]
======
Automatic Installation::
+
--
xref:installation/configuration.adoc#_configuration_with_install_script[インストールスクリプト]は、エアギャップインストールを行わない限り、互換性のあるシステムであればRancher RPMリポジトリからSELinux RPMを自動的にインストールします。自動インストールをスキップするには、``INSTALL_K3S_SKIP_SELINUX_RPM=true``を設定します。
--

Manual Installation::
+
--
必要なポリシーは以下のコマンドでインストールできます:

[,bash]
----
yum install -y container-selinux selinux-policy-base
yum install -y https://rpm.rancher.io/k3s/latest/common/centos/7/noarch/k3s-selinux-1.4-1.el7.noarch.rpm
----

インストールスクリプトが失敗するのではなく警告をログに記録するように強制するには、以下の環境変数を設定します: `INSTALL_K3S_SELINUX_WARN=true`。
--
======

=== SELinuxの強制モードを有効にする

SELinuxを活用するには、K3sサーバーおよびエージェントを起動する際に``--selinux``フラグを指定します。

このオプションはK3sのxref:installation/configuration.adoc#_configuration_file[設定ファイル]にも指定できます。

----
selinux: true
----

SELinuxの下でカスタムの``--data-dir``を使用することはサポートされていません。カスタマイズするには、おそらく独自のカスタムポリシーを書く必要があります。ガイダンスについては、コンテナランタイムのSELinuxポリシーファイルを含むlink:https://github.com/containers/container-selinux[containers/container-selinux]リポジトリおよびK3sのSELinuxポリシーを含むlink:https://github.com/k3s-io/k3s-selinux[k3s-io/k3s-selinux]リポジトリを参照してください。

== eStargzのレイジープルを有効にする（実験的機能）

=== レイジープルとeStargzとは？

イメージのプルはコンテナライフサイクルの中で時間のかかるステップの一つとして知られています。
https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter[Harter, et al.]によると、

____
パッケージのプルはコンテナ起動時間の76%を占めるが、そのデータのうち読み取られるのはわずか6.4%である
____

この問題に対処するために、k3sはイメージコンテンツの__レイジープル__を実験的にサポートしています。
これにより、k3sはイメージ全体がプルされる前にコンテナを起動することができます。
代わりに、必要なコンテンツのチャンク（例：個々のファイル）がオンデマンドで取得されます。
特に大きなイメージの場合、この技術はコンテナの起動遅延を短縮することができます。

レイジープルを有効にするには、ターゲットイメージをlink:https://github.com/containerd/stargz-snapshotter/blob/main/docs/stargz-estargz.md[_eStargz_]としてフォーマットする必要があります。
これはOCIの代替ですが、100% OCI互換のイメージフォーマットで、レイジープルに対応しています。
互換性があるため、eStargzは標準のコンテナレジストリ（例：ghcr.io）にプッシュでき、eStargz非対応のランタイムでも__実行可能__です。

eStargzはlink:https://github.com/google/crfs[Google CRFSプロジェクトによって提案されたstargzフォーマット]に基づいて開発されましたが、コンテンツの検証やパフォーマンスの最適化などの実用的な機能が追加されています。
レイジープルとeStargzの詳細については、https://github.com/containerd/stargz-snapshotter[Stargz Snapshotterプロジェクトリポジトリ]を参照してください。

=== eStargzのレイジープルのためのk3sの設定

以下のように、k3sサーバーおよびエージェントに``--snapshotter=stargz``オプションが必要です。

[,bash]
----
k3s server --snapshotter=stargz
----

この設定により、eStargz形式のイメージのレイジープルを実行できます。
以下の例のPodマニフェストは、eStargz形式の``node:13.13.0``イメージ（`ghcr.io/stargz-containers/node:13.13.0-esgz`）を使用しています。
stargzスナップショッタが有効になっている場合、K3sはこのイメージのレイジープルを実行します。

[,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: nodejs
spec:
  containers:
  - name: nodejs-estargz
    image: ghcr.io/stargz-containers/node:13.13.0-esgz
    command: ["node"]
    args:
    - -e
    - var http = require('http');
      http.createServer(function(req, res) {
        res.writeHead(200);
        res.end('Hello World!\n');
      }).listen(80);
    ports:
    - containerPort: 80
----

[#_additional_logging_sources]
== 追加のログソース

K3s用のlink:https://documentation.suse.com/cloudnative/rancher-manager/latest/en/observability/logging/logging-helm-chart-options.html[Rancherロギング]は、Rancherを使用せずにインストールできます。以下の手順を実行してください:

[,bash]
----
helm repo add rancher-charts https://charts.rancher.io
helm repo update
helm install --create-namespace -n cattle-logging-system rancher-logging-crd rancher-charts/rancher-logging-crd
helm install --create-namespace -n cattle-logging-system rancher-logging --set additionalLoggingSources.k3s.enabled=true rancher-charts/rancher-logging
----

== 追加のネットワークポリシーロギング

ネットワークポリシーによってドロップされたパケットをログに記録できます。パケットはiptablesのNFLOGアクションに送信され、パケットの詳細（ネットワークポリシーを含む）が表示されます。

トラフィックが多い場合、ログメッセージの数が非常に多くなる可能性があります。ポリシーごとにログのレートを制御するには、対象のネットワークポリシーに以下のアノテーションを追加して、``limit``および``limit-burst``のiptablesパラメータを設定します:

* `kube-router.io/netpol-nflog-limit=<LIMIT-VALUE>`
* `kube-router.io/netpol-nflog-limit-burst=<LIMIT-BURST-VALUE>`

デフォルト値は``limit=10/minute``および``limit-burst=10``です。これらのフィールドの形式および可能な値については、https://www.netfilter.org/documentation/HOWTO/packet-filtering-HOWTO-7.html#:~:text=restrict%20the%20rate%20of%20matches[iptablesマニュアル]を参照してください。

NFLOGパケットをログエントリに変換するには、ulogd2をインストールし、``[log1]``を``group=100``で読み取るように設定します。その後、ulogd2サービスを再起動して新しい設定を反映させます。
ネットワークポリシールールによってパケットがブロックされると、``/var/log/ulog/syslogemu.log``にログメッセージが表示されます。

NFLOGネットリンクソケットに送信されたパケットは、tcpdumpやtsharkなどのコマンドラインツールを使用して読み取ることもできます:

[,bash]
----
tcpdump -ni nflog:100
----

tcpdumpはより手軽に利用できますが、パケットをブロックしたネットワークポリシーの名前は表示されません。ネットワークポリシー名を含む完全なNFLOGパケットヘッダーを表示するには、wiresharkのtsharkコマンドを使用してください。

Network Policy logging of dropped packets does not support https://github.com/k3s-io/k3s/issues/8008[policies with an empty `podSelector`]. If you rely on logging dropped packets for diagnostic or audit purposes, ensure that your policies include a pod selector that matches the affected pods.
